import torch
import torch.nn.functional as F
import numpy as np

from dataloader import BatchedImages
from swin_transformer import SwinTransformer
from constants import IMG_SIZE, DECODER_LAYER_COUNT

class DepthAndUncertaintyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
         # args from https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation/blob/main/configs/swin/upernet_swin_tiny_patch4_window7_512x512_160k_ade20k.py
        self.backbone = SwinTransformer(
            embed_dim=96,
            depths=[2, 2, 6, 2],
            num_heads=[3, 6, 12, 24],
            window_size=7,
            ape=False,
            drop_path_rate=0.3,
            patch_norm=True,
            use_checkpoint=False

        )

        BACKBONE_OUT_DEPTH = 768 # Length of per-pixel information vector
        BACKBONE_OUT_SIZE = (60,80) # Size of image at highest resolution
        self.decoder_layer_depths = np.linspace(BACKBONE_OUT_DEPTH,2, num=DECODER_LAYER_COUNT+1).round().astype(int).tolist()
        self.layer_one, self.layer_two, *self.decoder_linear_layers = [torch.nn.Linear(self.decoder_layer_depths[i],self.decoder_layer_depths[i+1]) for i in range(DECODER_LAYER_COUNT)]
        self.decode_interp_sizes = np.linspace(BACKBONE_OUT_SIZE, IMG_SIZE, len(self.decoder_linear_layers)).round().astype(int).tolist()
    
    def apply_linear(self, linear, x):
        return linear(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
    def forward(self, x: BatchedImages):
        out = self.backbone(x.rgb)

        # These first three layers use the different size images the model emits 
        # We gradually scale up from the lowest resolution image, feeding in information from the other sizes as we go.
        x = out[-1] # torch.Size([16, 768, 20, 15])

        x = F.relu(self.apply_linear(self.layer_one, x))

        x= F.interpolate(x, mode="bilinear", size=(30, 40))

        #Generated by ChatGPT (to zeropad the feature vector to enable our extended neural network, while still being able to feed in the data from the smaller SWin default vector)
        out_shell = torch.zeros_like(x)
        slices = tuple(slice(0, s) for s in out[-2].size())
        out_shell[slices] = out[-2]

        x += out_shell

        x = F.relu(self.apply_linear(self.layer_two, x))

        x= F.interpolate(x, mode="bilinear", size=(60, 80))

        #Generated by ChatGPT
        out_shell = torch.zeros_like(x)
        slices = tuple(slice(0, s) for s in out[-3].size())
        out_shell[slices] = out[-3]

        x += out_shell

        # We have significantly more scaling up to do, for which we have a simple neural network.

        for i in range(len(self.decoder_linear_layers)):
            x = F.relu(self.apply_linear(self.decoder_linear_layers[i], x))
            x = F.interpolate(x, mode="bilinear", size=self.decode_interp_sizes[i])
    
        return x.permute(0, 2, 3, 1)
